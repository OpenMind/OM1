---
title: LLM Integration
description: "LLM Integration"
---

## LLM Integration

OM1's LLM integration is intended to make it easy to (1) send `input` information to LLMs and then (2) route LLM responses to various system `actions`, such as `speak` and `move`. The system provides a standardized interface for communicating with many different LLM endpoints from all the major providers, such as Anthropic, Google, and OpenAI. The plugins handle authentication, API communication, prompt formatting, response parsing, and conversation history management.

## LLM Code Location

LLM plugins are located in:

[github LLM](https://github.com/OpenmindAGI/OM1/tree/main/src/llm)

## How it Works

Each LLM plugin takes fused input data (the `prompt`) and sends it to an LLM (or a system of LLMs), and then waits for the response. The response is then parsed and provided to `runtime/cortex.py` for distribution to the system actions:

```python
response = await self._client.beta.chat.completions.parse(
    model=self._config.model,
    messages=[*messages, {"role": "user", "content": prompt}],
    response_format=self._output_model,
    timeout=self._config.timeout,
)

message_content = response.choices[0].message.content
parsed_response = self._output_model.model_validate_json(message_content)

return parsed_response
```

The standard `pydantic` output model is defined in `src/llm/output_model.py`.

## LLM Configuration

```json
# Example configuration
  "cortex_llm": {
    "type": "OpenAILLM",    // The class name of the LLM plugin you wish to use
    "config": {
      "base_url": "",       // Optional: URL of the LLM endpoint
      "agent_name": "Iris", // Optional: Name of the agent
      "history_length": 10  // The number of input->action cycles to provide to the LLM as historical context 
    }
  }
```