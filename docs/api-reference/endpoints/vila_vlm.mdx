---
title: VILA VLM
description: "Visual Language Model API Reference"
---

# VILA Visual Language Model API

VILA (Visual Intelligence and Language Analysis) is OpenMind's self-hosted Visual Language Model (VLM) API. This service enables real-time visual analysis and natural language understanding of video streams.

## Overview

The VILA VLM API provides:
- Real-time video stream analysis
- Natural language descriptions of visual content
- WebSocket-based communication for minimal latency
- Seamless integration with the OM1 package

## Connection

Connect to the API using WebSocket:

```bash
wss://api-vila.openmind.org?api_key=<YOUR_API_KEY>
```

## Installation

Install the OM1 package using either of these methods:

```bash
# Recommended: Install using uv (faster)
uv pip3 install git+https://github.com/OpenmindAGI/OM1.git

# Alternative: Install using pip
pip3 install git+https://github.com/OpenmindAGI/OM1.git
```

## Usage Example

Here's how to integrate VILA VLM into your application:

```python
import time
from om1_utils import ws
from om1_vlm import VideoStream

# Initialize VILA VLM client
ws_client = ws.Client(url="wss://api-vila.openmind.org?api_key=<YOUR_API_KEY>")
vlm = VideoStream(ws_client.send_message, fps=30)

# Start the service
ws_client.start()
vlm.start()

# Register callback for responses
ws_client.register_message_callback(lambda msg: print(msg))

# Keep the connection alive
while True:
    time.sleep(1)
```

## Response Format

The API returns JSON responses in this structure:

```json
{
  "vlm_reply": "The most interesting aspect in this series of images is the man's constant motion of speaking and looking in different directions while sitting in front of a laptop."
}
```

## Best Practices
- Ensure stable internet connection for WebSocket communication
- Adjust FPS based on your application's needs
- Handle WebSocket disconnections gracefully
- Process API responses asynchronously for optimal performance
