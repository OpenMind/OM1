---
title: Webcam Input
description: "Webcam input plugin"
---

---
title: Input Plugin Overview
description: "Input plugin overview"
---

## Input Plugin Overview

The Input Plugins in OM1 provide the sensory capabilities that allow AI agents to perceive and interact with their environment. These plugins capture, process, and format various types of input data from different sources, making them available to the agent's runtime core for decision-making.

[All the input plugins codes](https://github.com/OpenmindAGI/OM1/tree/main/src/inputs)

## Input Plugin Architecture

### Classes Diagram

In order to simplify the diagram, we only show the most important classes and their relationships.

```mermaid
classDiagram
    class SensorConfig {
        +__init__(**kwargs)
    }

    class Sensor~R~ {
        <<abstract>>
        +__init__(config: SensorConfig)
        +async _raw_to_text(raw_input: R) str
        +async raw_to_text(raw_input: R)
        +formatted_latest_buffer() str|None
        +async listen() AsyncIterator[R]
    }

    class VLMBase {
        +async _raw_to_text(raw_input: bytes) str
        +formatted_latest_buffer() str
    }

    class SerialReader {
        +async _listen_loop() AsyncIterator[bytes]
    }

    class GPSMagSerialReader {
        +async _raw_to_text(raw_input: bytes) str
    }

    class VLMCOCOLocal {
        +async _raw_to_text(raw_input: bytes) str
    }

    class VLMOpenAI {
        +async _raw_to_text(raw_input: bytes) str
    }

    class VLMVila {
        +async _raw_to_text(raw_input: bytes) str
    }

    class UnitreeGo2Camera {
        +async _listen_loop() AsyncIterator[bytes]
    }

    class UnitreeGo2LowState {
        +async _listen_loop() AsyncIterator[bytes]
    }

    SensorConfig --> Sensor
    Sensor <|-- VLMBase
    Sensor <|-- SerialReader
    SerialReader <|-- GPSMagSerialReader
    VLMBase <|-- VLMCOCOLocal
    VLMBase <|-- VLMOpenAI
    VLMBase <|-- VLMVila
    Sensor <|-- UnitreeGo2Camera
    Sensor <|-- UnitreeGo2LowState
```

- [class `Sensor`](https://github.com/OpenmindAGI/OM1/blob/8c73b76b54d1c379998be5f75a6b04e43e6a4701/src/inputs/base/__init__.py#L23)
- [class `FuserInput`](https://github.com/OpenmindAGI/OM1/blob/8c73b76b54d1c379998be5f75a6b04e43e6a4701/src/inputs/base/loop.py#L8)

### Data Flow Diagram

```mermaid
flowchart TD
    subgraph Input Sources
        A1[Camera Input]
        A2[Serial Port Input]
        A3[Robot State Input]
        A4[VLM Cloud Input]
    end

    subgraph Sensor Processing
        B1[VLM_COCO_Local]
        B2[GPSMagSerialReader]
        B3[UnitreeGo2LowState]
        B4[VLM Cloud Services]
    end

    subgraph Data Transformation
        C1[Raw to Text Conversion]
        C2[Message Formatting]
        C3[Buffer Management]
    end

    subgraph Orchestration
        D1[InputOrchestrator]
        D2[Async Task Management]
    end

    subgraph Output
        E1[Formatted Messages]
        E2[Text Stream]
    end

    %% Input Sources to Sensors
    A1 --> B1
    A2 --> B2
    A3 --> B3
    A4 --> B4

    %% Sensor Processing
    B1 --> C1
    B2 --> C1
    B3 --> C1
    B4 --> C1

    %% Data Transformation
    C1 --> C2
    C2 --> C3

    %% Orchestration
    C3 --> D1
    D1 --> D2

    %% Output
    D2 --> E1
    D2 --> E2

    %% Detailed Data Flow for VLM_COCO_Local
    subgraph VLM_COCO_Local Flow
        F1[Image Capture]
        F2[Object Detection]
        F3[Position Analysis]
        F4[Message Generation]
    end

    F1 --> F2
    F2 --> F3
    F3 --> F4
    F4 --> C1
```
## Key relationships and notes:

### Inheritance Hierarchy

- `Sensor<T>` is the base abstract class that defines the core interface
- `FuserInput<T>` extends `Sensor<T>` and implements the polling mechanism
- `VLM_COCO_Local` extends `FuserInput<T>` and implements the specific VLM functionality

### Key Components

- Uses PyTorch's `FasterRCNN_MobileNet_V3_Large_320_FPN` model
- Processes images from webcam or other sources
- Detects objects using COCO dataset classes
- Provides spatial awareness (left/right/front)

### Key Functionality

- Real-time object detection
- Spatial object localization
- Message buffering and formatting
- Webcam integration

## Other Input Plugins

- [Google ASR](https://github.com/OpenmindAGI/OM1/blob/main/src/inputs/plugins/google_asr.py)
- [Riva ASR](https://github.com/OpenmindAGI/OM1/blob/main/src/inputs/plugins/riva_asr.py)
- [RPLidar](https://github.com/OpenmindAGI/OM1/blob/main/src/inputs/plugins/rplidar.py)
- [VLM_COCO_Local](https://github.com/OpenmindAGI/OM1/blob/main/src/inputs/plugins/vlm_coco_local.py)
- [VLM_Villa](https://github.com/OpenmindAGI/OM1/blob/main/src/inputs/plugins/vlm_vila.py)
- [Arduino GPS](https://github.com/OpenmindAGI/OM1/blob/main/src/inputs/plugins/gps_mag_serial_reader.py)

## Webcam Input Plugin

The Webcam Input Plugin in OM1 provides the ability to generate texts from a webcam. This plugin captures images from a webcam, translates into texts and makes them available to the agent's runtime core for decision-making.

Let's take VLM Villa as an example.
[VLM Villa input plugin code](https://github.com/OpenmindAGI/OM1/blob/main/src/inputs/plugins/vlm_vila.py)

## Vision Components

### Class Diagram

In order to simplify the diagram, we only show the most important classes and their relationships.

```mermaid
classDiagram
    class FuserInput~T~ {
        <<abstract>>
        +async _poll() T
        +async _raw_to_text(raw_input: T) str
        +async raw_to_text(raw_input: T)
        +formatted_latest_buffer() str|None
    }

    class Message {
    }

    class VLMVila {
        +__init__(config: SensorConfig)
        -_handle_vlm_message(raw_message: str)
        +async _poll() str|None
        +async _raw_to_text(raw_input: str) Message
        +async raw_to_text(raw_input: str|None)
        +formatted_latest_buffer() str|None
    }

    class VLMVilaProvider {
        +start()
        +register_message_callback(callback)
    }

    class IOProvider {
        +add_input(descriptor: str, message: str, timestamp: float)
    }

    FuserInput <|-- VLMVila
    VLMVila --> Message
    VLMVila --> VLMVilaProvider
    VLMVila --> IOProvider
```

### Data Flow

```mermaid
sequenceDiagram
    participant Client
    participant VLMVila
    participant VLMVilaProvider
    participant MessageBuffer
    participant IOProvider

    Note over VLMVila: Initialize with config
    VLMVila->>VLMVilaProvider: Initialize with API settings
    VLMVilaProvider->>VLMVila: Start VLM service
    
    loop Continuous Vision Processing
        Client->>VLMVilaProvider: Send image
        VLMVilaProvider->>VLMVila: _handle_vlm_message(raw_message)
        VLMVila->>MessageBuffer: Store valid messages
        
        loop Polling
            VLMVila->>MessageBuffer: _poll()
            MessageBuffer-->>VLMVila: Return message if available
            VLMVila->>VLMVila: _raw_to_text()
            VLMVila->>VLMVila: raw_to_text()
            VLMVila->>IOProvider: formatted_latest_buffer()
        end
    end
```

### Example configuration

```json agent_inputs
  "agent_inputs": [
    {
      "type": "VLMVila",
      "config": {
        "base_url": "wss://api-vila.openmind.org"
      }
    }
  ]
```

### configuration parameters

Here is an example of how to configure the VLM Villa input plugin:

```
# Example usage
config = SensorConfig(
    api_key="your_api_key",
    base_url="wss://api-vila.openmind.org",
    stream_base_url="wss://api.openmind.org/api/core/teleops/stream"
)
vlm = VLMVila(config)
```

---
title: Speech Recognition
description: "Speech recognition input plugin"
---
 
## Speech Recognition Input Plugin

The Speech Recognition Input Plugin in OM1 provides the ability to recognize and transcribe spoken language. This plugin captures audio input from a microphone and converts it into text, making it available to the agent's runtime core for decision-making.

## ASR Components

The ASR components are responsible for capturing audio input from a microphone and converting it into text. The main components are (take GoogleASRInput as an example):

[google asr input plugin code](https://github.com/OpenmindAGI/OM1/blob/main/src/inputs/plugins/google_asr.py)

### Class Diagram

In order to simplify the diagram, we only show the most important classes and their relationships.

```mermaid
classDiagram
    class FuserInput~T~ {
        <<abstract>>
        +async _poll() T
        +async _raw_to_text(raw_input: T) str
        +async raw_to_text(raw_input: T)
        +formatted_latest_buffer() str|None
    }

    class GoogleASRInput {
        +__init__(config: SensorConfig)
        -_handle_asr_message(raw_message: str)
        +async _poll() str|None
        +async _raw_to_text(raw_input: str) str
        +async raw_to_text(raw_input: str)
        +formatted_latest_buffer() str|None
    }

    class ASRProvider {
        +start()
        +register_message_callback(callback)
    }

    class IOProvider {
        +add_input(descriptor: str, message: str, timestamp: float)
    }

    class SleepTickerProvider {
    }

    FuserInput <|-- GoogleASRInput
    GoogleASRInput --> ASRProvider
    GoogleASRInput --> IOProvider
    GoogleASRInput --> SleepTickerProvider
```

### Data Flow

Speech Recognition Input Plugin Data Flow:

```mermaid
sequenceDiagram
    participant User
    participant GoogleASRInput
    participant ASRProvider
    participant MessageBuffer
    participant IOProvider

    Note over GoogleASRInput: Initialize with config
    GoogleASRInput->>ASRProvider: Initialize with language & settings
    ASRProvider->>GoogleASRInput: Start ASR service
    
    loop Continuous Speech Recognition
        User->>ASRProvider: Speak
        ASRProvider->>GoogleASRInput: _handle_asr_message(raw_message)
        GoogleASRInput->>MessageBuffer: Store valid messages
        
        loop Polling
            GoogleASRInput->>MessageBuffer: _poll()
            MessageBuffer-->>GoogleASRInput: Return message if available
            GoogleASRInput->>GoogleASRInput: _raw_to_text()
            GoogleASRInput->>GoogleASRInput: raw_to_text()
            GoogleASRInput->>IOProvider: formatted_latest_buffer()
        end
    end
```

### Example configuration

```json agent_inputs
  "agent_inputs": [
    {
      "type": "GoogleASRInput",
      "config": {
        "language": "en-US"
      }
    }
  ]
```

### configuration parameters

Here is an example of how to configure the ASR input plugin:
```
# Example usage
config = SensorConfig(
    api_key="your_api_key",
    language="en-US",
    rate=48000,
    chunk=12144,
    microphone_name="your_mic"
)
asr_input = GoogleASRInput(config)
```