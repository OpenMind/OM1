---
title: Core Architecture and Runtime Flow
description: "Core Architecture and Runtime Flow"
---

## Project Structure

```tree Project Structure
.
├── config/               # Agent configuration files
├── src/
│   ├── actions/          # Agent outputs/actions/capabilities
│   ├── fuser/            # Input fusion logic
│   ├── inputs/           # Input plugins (e.g. VLM, audio)
│   ├── llm/              # LLM integration
│   ├── providers/        # Background tasks
│   ├── runtime/          # Core runtime system
│   ├── simulators/       # Virtual endpoints such as `WebSim`
│   ├── zenoh_idl/        # Zenoh's Interface Definition Language (IDL)
│   └── run.py            # CLI entry point
```

The system is based on a loop that runs at a fixed frequency of `self.config.hertz`. The loop looks for the most recent data from various sources, fuses the data into a prompt, sends that prompt to one or more LLMs, and then sends the LLM responses to virtual agents or physical robots.

## Architecture Overview

The system architecture is shown below:

![](/assets/om1-architecture.png)

The architecture diagram illustrates a comprehensive OM1's distinct layers and modules. Here’s a detailed breakdown:

### Sensor Layer

Sensors provide raw inputs:

- Vision: Cameras for visual perception.
- Sound: Microphones capturing audio data.
- Battery/System: Monitoring battery and system health.
- Location/GPS: Positioning information.
- LIDAR: Laser-based sensing for 3D mapping and navigation.

### AI and Conversational World Captioning Layer

Processes raw sensor data into meaningful descriptions:

- VLM (Vision Language Model): Converts visual data to natural language descriptions (e.g., human activities, object interactions).
- ASR (Automatic Speech Recognition): Converts audio data into textual representation.
- Platform State: Describes internal system status (e.g battery percentage, odometry readings).
- Spatial/NAV: Processes location and navigation data.
- 3D environments: Interprets 3D environmental data from sensors like LIDAR.

### Natural Language Data Bus (NLDB)

A centralized bus that collects and manages natural language data generated from various captioning modules, ensuring structured data flow between components. Example outputs include:

```bash
Vision: “You see a human. He looks happy and is smiling and pointing to a chair.”
Sound: “You just heard: Bits, run to the chair.”
Odom: 1.3, 2.71, 0.32
Power: 73%
```

### Data Fuser

This module combines short inputs from the NLDB into paragraphs, providing context and situational awareness to subsequent decision-making modules. It fuses spatial data (e.g., number and location of proximal humans and robots), audio commands, and visual cues into a unified world description.

Example fused data:

```
137.0270: You see a human, 3.2 meters to your left. He looks happy and is smiling. He is pointing to a chair. You just heard: Bits run to the chair.
139.0050: You see a human, 1.5 meters in front of you. He is showing you a flat hand. You just heard: Bits, stop.
```

### Multi AI Planning/Decision Layer

Uses fused data to make decisions through one or more AI models:

- Fast Action LLM (Local or Cloud): A small LLM that quickly processes immediate or time-critical actions without significant latency. Expected token response time - 300 ms.
- Cognition ("Core") LLM (Cloud): Cloud-based LLM for complex reasoning, long-term planning, and high-level cognitive tasks, leveraging more computational resources. Expected token response time - 2 s.
- Mentor/Coach LLM (Cloud): Cloud-based LLM for 3rd person view critique of the robot-human interaction. Generates full critique every 30 seconds and provides it to the Core LLM.

These LLMs are constrained by natural language rules provided in the configuration files, or, downloaded from immutable public ledgers (blockchains) such as Ethereum. Storing robot constitutions/guardrails on immutable public ledgers facilitates transparency, traceability, decentralized coordination, and logging for accountability.

Feedback Loop:
- Adjustments based on performance metrics or environmental conditions (e.g., adjusting vision frame rates for efficiency).

### Hardware Abstraction Layer (HAL)

This layer translates high-level AI decisions into actionable commands for robot hardware:
- Move: Controls robot movement.
- Sound: Generates auditory signals.
- Speech: Handles synthesized voice outputs.
- Wallet: Digital wallet for economic transactions or cryptographic operations for identity verification.

### Overall System Data Flow

Robot Sensors → AI Captioning (Audio, LIDAR, Spatial RAG, Vision models, "Foundational" Models) → NLDB → Data Fuser → AI Decision Layer (Emergency Responder LLM, Core LLM, Coach LLM) → HAL → Robot Actions (movement policies, Action models)


