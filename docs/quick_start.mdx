---
title: 'Quick Start'
description: 'Get started with OpenMind OS (OM1) in minutes'
---

<Note>
  This guide will help you set up and run your first OM1 agent. We'll use the `Spot` agent, which demonstrates basic vision, speech, and movement capabilities.
</Note>

## Prerequisites

<CardGroup cols={2}>
  <Card title="Python 3.10+" icon="python">
    OM1 requires Python 3.10 or newer
  </Card>
  <Card title="Git" icon="git-alt">
    For cloning the repository and managing updates
  </Card>
</CardGroup>

## Installation

<Steps>
  <Step title="Clone the Repository">
    ```bash
    git clone https://github.com/OpenmindAGI/OM1.git
    cd OM1
    git submodule update --init
    ```
  </Step>

  <Step title="Set Up Python Environment">
    Install the UV package manager:
    <CodeGroup>
      ```bash Mac
      brew install uv
      ```
      ```bash Linux
      curl -LsSf https://astral.sh/uv/install.sh | sh
      ```
    </CodeGroup>

    Then create a virtual environment:
    ```bash
    uv venv
    ```
  </Step>

  <Step title="Configure API Keys">
    Add your OpenMind API key in `/config/spot.json`:
    ```json
    {
      "api_key": "openmind_om1_pat_2f1cf005af........."
    }
    ```
    <Note>
      Get your free access key at [portal.openmind.org](https://portal.openmind.org/). 
      Using the placeholder `openmind-free` may result in rate limiting.
    </Note>
  </Step>
</Steps>

## Running Your First Agent

<Steps>
  <Step title="Start the Spot Agent">
    ```bash
    uv run src/run.py spot
    ```
  </Step>

  <Step title="Check the Output">
    You should see logging information like:
    ```bash
    INFO:root:SendThisToROS2: {'move': 'dance'}
    INFO:root:SendThisToROS2: {'speak': "Hello, it's so nice to see you! Let's dance together!"}
    INFO:root:SendThisToROS2: {'face': 'joy'}
    INFO:root:VLM_COCO_Local: You see a person in front of you.
    ```
  </Step>

  <Step title="View Debug Interface">
    Open [http://localhost:8000](http://localhost:8000) to see:
    - Real-time inputs and outputs
    - System latency metrics
    - Agent state information
  </Step>
</Steps>

## Understanding the Output

<CodeGroup>
  ```json Movement
  {
    "move": "dance"
  }
  ```
  ```json Speech
  {
    "speak": "Hello, it's so nice to see you! Let's dance together!"
  }
  ```
  ```json Expression
  {
    "face": "joy"
  }
  ```
</CodeGroup>

The agent processes inputs and generates three types of outputs:
- Movement commands
- Speech responses
- Facial expressions

## Advanced Usage

<Accordion title="Debug Mode">
  Add `--debug` for detailed logging:
  ```bash
  uv run src/run.py spot --debug
  ```
</Accordion>

<Accordion title="Custom Configuration">
  Modify `config/spot.json` to:
  - Adjust response frequency (`hertz`)
  - Change system prompts
  - Configure input sources
  - Add new actions
</Accordion>

<Accordion title="Dependencies">
  `uv` handles:
  - Virtual environment creation
  - Package installation
  - Dependency resolution
  
  Add new packages to `pyproject.toml`
</Accordion>

## Common Issues

<CardGroup cols={2}>
  <Card title="Audio Setup" icon="volume-high">
    The simulator shows generated speech but doesn't play audio by default, reducing audio driver requirements.
  </Card>
  
  <Card title="Model Loading" icon="spinner">
    Complex models or first-time dependency downloads may cause initial delays.
  </Card>
</CardGroup>

## Next Steps

<CardGroup cols={2}>
  <Card
    title="Developer Guide"
    icon="code"
    href="/docs/development/guide"
  >
    Learn how to build custom agents
  </Card>
  
  <Card
    title="Configuration"
    icon="gear"
    href="/docs/development/configuration"
  >
    Explore configuration options
  </Card>
  
  <Card
    title="Examples"
    icon="lightbulb"
    href="/docs/examples"
  >
    See more agent examples
  </Card>
  
  <Card
    title="Robotics"
    icon="robot"
    href="/docs/robotics"
  >
    Deploy to physical robots
  </Card>
</CardGroup>

<Note>
  Need help? Join our [Discord community](https://discord.gg/openmind) or check out our [GitHub discussions](https://github.com/OpenmindAGI/OM1/discussions).
</Note>
