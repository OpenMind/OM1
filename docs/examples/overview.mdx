---
title: Tutorials Overview
description: "List of tutorials for OM1"
---

This section provides a comprehensive overview of the tutorials available in the OM1 documentation. Each tutorial is designed to help you understand and implement various features and functionalities of OM1.

## Tutorials list

All the implementation configurations of the tutorials are available in the [OM1 GitHub repository](https://github.com/OpenmindAGI/OM1/tree/main/config).

- conversation.json5
- cubly.json5
- deepseek.json5
- gemini.json5
- grok.json5
- healthy.json5
- multiagent.json5
- open_ai.json5
- quadruped_sim.json5
- rag_multiagent.json5
- robot_wallet_safe.json5
- spot.json5
- tesla.json5
- turtlebot4.json5
- turtlebot4_lidar.json5
- turtlebot4_old.json5
- twitter.json5
- unitree_g1_humanoid.json5
- unitree_go2.json5
- unitree_go2_lidar.json5
- unitree_go2_remote.json5

## Configuration files

Each tutorial has its own configuration file. Let's go through the configuration files section by section.
There are 4 main sections in the configuration files:

- agent_inputs
- cortex_llm
- agent_actions
- simulators

Refer to the [Configuration Files](/getting-started/configuration) section for more information.

### agent_inputs

Refer to the [Agent Inputs plugin](/input-plugin/input-plugin-overview) section for more information.

There are multiple types of agent_inputs in the configuration files. Let's go through them.

**DIMO**

- DIMOTesla - Read Tesla data from DIMO API and use it as an agent input

**Vision**
- FaceEmotionCapture - Use Face Emotion Capture as an agent input
- VLMVila - Use VLM Vila as an agent input
- VLM_COCO_Local - Use VLM COCO Local as an agent input

**Audio**
- GoogleASRInput - Use Google ASR as an agent input

**Lidar**
- RPLidar - Use RPLidar to read LiDAR data as an agent input

**GPS**
- SerialReader - Use Serial Reader to read GPS data as an agent input

**Robot**
- TurtleBot4Batt - Use TurtleBot4 Batt as an agent input
- UnitreeG1Basic - Use Unitree G1 Basic as an agent input
- UnitreeGo2CameraVLMCloud - Use Unitree Go2 Camera VLM Cloud as an agent input
- UnitreeGo2Lowstate - Use Unitree Go2 Lowstate as an agent input

**Misc**
- GovernanceEthereum - Use Governance rules for robots on Ethereum as an agent input
- TwitterInput - Use Twitter Input specified in the config as an agent input

### cortex_llm

Refer to the [Cortex LLM integration](/llm/llm-integration) section for more information.

OM1 supports multiple LLMs models:

- OpenAILLM - openai GPT 4o, GPT 4o mini
- DeepSeekLLM - DeepSeek Chats
- GeminiLLM - gemini-2.0-flash-exp
- XAILLM - grok-3-mini-beta
- MultiLLM - multi-agent LLM with openai GPT 4o, GPT 4o mini, GPT 4.1, GPT 4.1-mini, GPT 4.1-nano

### agent_actions

Refer to the [Agent Actions plugin](/action-plugin/action-plugin-overview) section for more information.

There are 2 main types of agent_actions:

- Movement
- Speech

For movement, refer to the [Movement plugin via ROS2](/action-plugin/movement-ros2) section for more information, or the [Movement plugin via zenoh](/action-plugin/movement-zenoh) section for more information.

For speech, refer to the [Speech plugin](/action-plugin/speech-and-tts) section for more information.

### simulators

- WebSim - Use WebSim as an agent input

## Customize your robot

