---
title:  Quick Start
description: "Run your first OM1-powered robot"
---

## Prerequisites

- api key from [OM1 Portal](https://portal.openmind.org/)
- All the prerequisites from the [Installation Guide](/getting-started/get-started)

## Run the Spot Agent

Run the following command to start the Spot Agent:

```bash
uv run src/run.py spot
```

## WebSim to check input and output

Go to [http://localhost:8000](http://localhost:8000) to see real time logs along with the input and output in the terminal. For easy debugging, add `--debug` to see additional logging information.

**Congratulations!** - you just got started with OM1 and can now explore its capabilities.

Some necessary packages will be installed during this process, the first time you run the command. This might take a little time. Please be patient. Then you will see the system come to life:

```bash
Object Detector INPUT
// START
You see a person in front of you. You also see a laptop.
// END

AVAILABLE ACTIONS:
command: move
    A movement to be performed by the agent.
    Effect: Allows the agent to move.
    Arguments: Allowed values: 'stand still', 'sit', 'dance', 'shake paw', 'walk', 'walk back', 'run', 'jump', 'wag tail'

command: speak
    Words to be spoken by the agent.
    Effect: Allows the agent to speak.
    Arguments: <class 'str'>

command: emotion
    A facial expression to be performed by the agent.
    Effect: Performs a given facial expression.
    Arguments: Allowed values: 'cry', 'smile', 'frown', 'think', 'joy'

What will you do? Command: 

INFO:httpx:HTTP Request: POST https://api.openmind.org/api/core/openai/chat/completions "HTTP/1.1 200 OK"
INFO:root:OpenAI LLM output: commands=[Command(type='move', value='wag tail'), Command(type='speak', value="Hi there! I see you and I'm excited!"), Command(type='emotion', value='joy')]
```

### Explanation of the log data

The response above provides insight into how the spot agent processes its environment and decides on its next actions.
  - First, it detects a person using vision.
  - Next, it decides on a friendly action (dancing and speaking).
  - Expresses emotions via facial displays.
  - Logs latency and processing times to monitor system performance.
  - Communicates with an external AI API for response generation.

Overall, the system follows a predefined behavior where spotting a person triggers joyful interactions, driven by the LLM-assisted decision-making process. 

## More examples

There are some pre-configured agents in the `/config` folder, you can run them by the following command:

```bash
uv run src/run.py <agent_name>
```

For example, to run the `conversation` agent, you can use the following command:

```bash
uv run src/run.py conversation
```

