---
title: Configuration
description: "Detailed configuration explanation"
---

## Configuration Example

Agents are configured via JSON5 files in the `/config` directory. 

The configuration file is used to define the LLM `system prompt`, agent's inputs, LLM configuration, and actions etc.

Here is an example of the configuration file:

```json Configuration
{
  "hertz": 0.5,
  "name": "agent_name",
  "api_key": "openmind_free",
  "URID": "default",
  "system_prompt_base": "...",
  "system_governance": "...",
  "system_prompt_examples": "...",
  "agent_inputs": [
    {
      "type": "GovernanceEthereum"
    },
    {
      "type": "VLM_COCO_Local",
      "config": {
        "camera_index": 0
      }
    }
  ],
  "cortex_llm": {
    "type": "OpenAILLM",
    "config": {
      "base_url": "", // Optional: URL of the LLM endpoint
      "agent_name": "Iris", // Optional: Name of the agent
      "history_length": 10
    }
  },
  "simulators": [
    {
      "type": "WebSim",
      "config": {
        "host": "0.0.0.0",
        "port": 8000,
        "tick_rate": 100,
        "auto_reconnect": true,
        "debug_mode": false
      }
    }
  ],
  "agent_actions": [
    {
      "name": "move",
      "llm_label": "move",
      "implementation": "passthrough",
      "connector": "ros2"
    },
    {
      "name": "speak",
      "llm_label": "speak",
      "implementation": "passthrough",
      "connector": "ros2"
    }
  ]
}
```

## Common Configuration Elements

* **hertz** Defines the base tick rate of the agent. This rate can be overridden to allow the agent to respond quickly to changing environments using event-triggered callbacks through real-time middleware.

* **name** A unique identifier for the agent.

* **api_key** The API key for the agent. You can get your API key from the [OpenMind Portal](https://portal.openmind.org/).

* **URID** The Universal Robot ID for the robot. Used to join a decentralized machine-to-machine coordination and communication system (FABRIC). 

* **system_prompt_base** Defines the agent's personality and behavior. This acts as the system prompt for the agent's operations.

* **system_governance** Defines the agent's governance. This acts as the governance prompt for the agent's operations.

* **system_prompt_examples** Defines the agent's examples. This acts as the examples prompt for the agent's operations.

## agent_inputs

Example configuration for the agent_inputs section:

```json agent_inputs
  "agent_inputs": [
    {
      "type": "GovernanceEthereum"
    },
    {
      "type": "VLM_COCO_Local",
      "config": {
        "camera_index": 0
      }
    }
  ]
```

The agent_inputs section defines the inputs for the agent. The input might include a camera, a LiDAR, a microphone, or governance information.  
OM1 implemented the following input types as reference:

* GovernanceEthereum
* GoogleASRInput
* VLMVila
* VLM_COCO_Local
* RPLidar
* TurtleBot4Batt
* UnitreeG1Basic
* UnitreeGo2Lowstate
* more is coming soon...

Definitely you can implement your own input by following the [Input Plugin](https://github.com/openmind/OM1/tree/main/src/inputs/plugins)

### agent_inputs config section

The config section is specific to the input type. For example, the `VLM_COCO_Local` input type has a config section that includes a `camera_index` parameter.

## cortex_llm

**cortex_llm** is for the Large Language Model (LLM) used by the agent.

Example configuration for the `cortex_llm` section:

```json cortex_llm
  "cortex_llm": {
    "type": "OpenAILLM",
    "config": {
      "base_url": "", // Optional: URL of the LLM endpoint
      "api_key": "...",   // Optional: Override the default API key
      "agent_name": "Iris", // Optional: Name of the agent
      "history_length": 10
    }
  }
```

* **type**: Specifies the LLM plugin.

* **config**: Configuration for the LLM, including the API endpoint (base_url), agent_name, and history_length.

Read more information about [Openmind API Reference](/api-reference/introduction).

You can directly access other OpenAI style endpoints by specifying a custom API endpoint in your configuration file. To do this, provide a suitable `base_url` and the `api_key` for OpenAI, DeepSeek, or other providers. Possible `base_url` choices are:

* https://api.openai.com/v1
* https://api.deepseek.com/v1

## simulators

Lists the simulation modules used by the agent. These define the simulated environment or entities the agent interacts with.

Example configuration for the `simulators` section:

```json Simulators Configuration
  "simulators": [
    {
      "type": "WebSim",
      "config": {
        "host": "0.0.0.0",
        "port": 8000,
        "tick_rate": 100,
        "auto_reconnect": true,
        "debug_mode": false
      }
    }
  ]
```

## agent_actions

Defines the agent's available capabilities, including action names, their implementation, and the connector used to execute them.

Example configuration for the `agent_actions` section:

```json Agent Actions Configuration
  "agent_actions": [
    {
      "name": "move",
      "llm_label": "move",
      "implementation": "passthrough",
      "connector": "ros2"
    },
    {
      "name": "speak",
      "llm_label": "speak",
      "implementation": "passthrough",
      "connector": "ros2"
    }
  ]
```

* **name**: The name of the action.

* **llm_label**: The label of the action.

* **implementation**: The implementation of the action.

* **connector**: The connector of the action.

You can customize the action by following the [Action Plugin](https://github.com/openmind/OM1/tree/main/src/actions)
